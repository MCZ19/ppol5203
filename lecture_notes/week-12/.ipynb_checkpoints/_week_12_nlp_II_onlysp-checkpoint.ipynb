{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e380a95",
   "metadata": {},
   "source": [
    "<h1><center> PPOL 5203 Data Science I: Foundations <br><br> \n",
    "<font color='grey'> Working with Text as Data<br><br>\n",
    "Tiago Ventura </center> <h1> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4eed89",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "This notebook will cover: \n",
    "\n",
    "- Supervised: Sentiment Analysis\n",
    "    - Dictionary\n",
    "    - ML for Text-Classification\n",
    "    - Working with Pre-trained Models - Transformers\n",
    "    - Outsourcing to Generative Text-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c2b30",
   "metadata": {},
   "source": [
    "## Supervised Learning with Text\n",
    "\n",
    "To practice with supervised learning with text data, we will perform some classic sentiment analysis classification task. Sentiment analysis natural language processing technique that given a textual input (tweets, movie reviews, comments on a website chatbox, etc... ) identifies the polarity of the text. \n",
    "\n",
    "There are different flavors of sentiment analysis, but one of the most widely used techniques labels data into positive, negative and neutral. Other options are classifying text according to the levels of toxicity, which I did in the paper I asked you to read, or more fine-graine measures of sentiments. \n",
    "\n",
    "Sentiment analysis is just one of many types of classification tasks that can be done with text. For any type of task in which you need to identify if the input pertains to a certain category, you can use a similar set of tools as we will see for sentiment analysis. For example, these are some classification tasks I have used in my work before: \n",
    "\n",
    "- Classify the levels of toxicity in social media live-streaming comments.\n",
    "- Analyze the sentiment of tweets.\n",
    "- Classify if the user is a Republican or Democrat  given the their Twitter bios. \n",
    "- Identify if a particular social media post contains misinformation. \n",
    "\n",
    "For all these tasks, you need: \n",
    "\n",
    "- some type of labelled data (which you and your research team will do), \n",
    "- build/or use a pre-trained machine learning models to make the prediction\n",
    "- evaluate the performance of the models\n",
    "\n",
    "Here, we will work with data that was alread labelled for us. We will analyze the sentiment on IMDB dataset of reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60892ec4",
   "metadata": {},
   "source": [
    "### IMDB Dataset\n",
    "\n",
    "For the rest of this notebook, we will IMDB dataset provided by [Hugging Face](https://huggingface.co/datasets/imdb). The IMDB dataset contains 25,000 movie reviews labeled by sentiment for training a model and 25,000 movie reviews for testing it. \n",
    "\n",
    "We will talk more about the Hugging Face project later in this notebook. For now, just download their main transformers library, and import the IMDB Review Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97a12b",
   "metadata": {},
   "source": [
    "#### Accessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de00de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7824ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -q transformers\n",
    "from datasets import load_dataset\n",
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ce9e8",
   "metadata": {},
   "source": [
    "#### get a smaller sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c891571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = imdb[\"train\"].shuffle(seed=42).select([i for i in list(range(3000))])\n",
    "small_test_dataset = imdb[\"test\"].shuffle(seed=42).select([i for i in list(range(300))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ef88fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is no relation at all between Fortier an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie is a great. The plot is very true t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>George P. Cosmatos' \"Rambo: First Blood Part I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the process of trying to establish the audi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yeh, I know -- you're quivering with excitemen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  There is no relation at all between Fortier an...      1\n",
       "1  This movie is a great. The plot is very true t...      1\n",
       "2  George P. Cosmatos' \"Rambo: First Blood Part I...      0\n",
       "3  In the process of trying to establish the audi...      1\n",
       "4  Yeh, I know -- you're quivering with excitemen...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to a dataframe\n",
    "pd_train = pd.DataFrame(small_train_dataset)\n",
    "pd_test = pd.DataFrame(small_test_dataset)\n",
    "\n",
    "# see the data\n",
    "pd_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7bb5f6",
   "metadata": {},
   "source": [
    "### Dictionary Methods\n",
    "\n",
    "Our first approach for sentiment classification will use dictionary methods. \n",
    "\n",
    "**Common Procedure:** Consists on using a pre-determined set of words (dictionary) that identifies the categories you want to classify documents. With this dictionary, you can do a simple search through the documents, count how many times these words appear, and use some type of aggregation function to classify the text. For example: \n",
    "\n",
    "- Positive or negative, for sentiment\n",
    "- Sad, happy, angry, anxious... for emotions\n",
    "- Sexism, homophobia, xenophobia, racism... for hate speech\n",
    "\n",
    "Dictionaries are the most basic strategy to classify documents. Its simplicity requires some unrealistic assumptions (for example related to ignoring contextual information of the documents). However, the use of dicitionaries have one major advantage: it allows for a bridge between qualititative and quantitative knowledge. You need human experts to build good dictionaries.  \n",
    "\n",
    "#### VADER\n",
    "\n",
    "There are many options for dictionaries for sentiment classification. We will use one popular open-source option available at NLTK: The VADER dictionary. VADER stands for Valence Aware Dictionary for Sentiment Reasoning. It is a model used for text sentiment analysis that is sensitive to both polarity (positive/negative) and intensity (strength) of emotion, and it was developed to handling particularly social media content. \n",
    "\n",
    "\n",
    "**Key Components of the VADER Dictionary:***\n",
    "\n",
    "- Sentiment Lexicon: This is a list of known words and their associated sentiment scores. \n",
    "\n",
    "- Sentiment Intensity Scores: Each word in the lexicon is assigned a score that ranges from -4 (extremely negative) to +4 (extremely positive). \n",
    "\n",
    "- Handling of Contextual and Qualitative Modifiers: VADER is sensitive to both intensifiers (e.g., \"very\") and negations (e.g., \"not\"). \n",
    "\n",
    "You can read the original paper that created the VADER [here](https://www.google.com/search?q=ADER%3A+A+Parsimonious+Rule-based+Model+for+Sentiment+Analysis+of+Social+Media+Text.+Eighth+International+Conference+on+Weblogs+and+Social+Media&rlz=1C5GCEM_enUS1072US1073&oq=ADER%3A+A+Parsimonious+Rule-based+Model+for+Sentiment+Analysis+of+Social+Media+Text.+Eighth+International+Conference+on+Weblogs+and+Social+Media&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIGCAEQRRg60gEHMTU2ajBqNKgCALACAA&sourceid=chrome&ie=UTF-8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21827296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Import dictionary\n",
    "import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c85cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97dd8e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.544, 'pos': 0.456, 'compound': 0.8553}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple example\n",
    "review1 = \"Oh, I loved the Data Science I course. The best course I have ever done!\"\n",
    "\n",
    "# classify\n",
    "sid.polarity_scores(review1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b44237c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.244, 'neu': 0.445, 'pos': 0.311, 'compound': -0.0243}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple example\n",
    "review2 = \"DS I was ok. Professor Ventura jokes were not great\"\n",
    "\n",
    "# classify\n",
    "sid.polarity_scores(review2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac449848",
   "metadata": {},
   "source": [
    "Let's now apply the dictionary at scale in our IMDB review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9376cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the dictionary to your data frame\n",
    "pd_test[\"vader_scores\"]=pd_test[\"text\"].apply(sid.polarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cfc2d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>vader_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;When I unsuspectedly rented A Thou...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.069, 'neu': 0.788, 'pos': 0.143, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is the latest entry in the long series of...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.066, 'neu': 0.862, 'pos': 0.073, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie was so frustrating. Everything seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'neg': 0.24, 'neu': 0.583, 'pos': 0.177, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was truly and wonderfully surprised at \"O' B...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.075, 'neu': 0.752, 'pos': 0.173, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This movie spends most of its time preaching t...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'neg': 0.066, 'neu': 0.707, 'pos': 0.227, 'co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  <br /><br />When I unsuspectedly rented A Thou...      1   \n",
       "1  This is the latest entry in the long series of...      1   \n",
       "2  This movie was so frustrating. Everything seem...      0   \n",
       "3  I was truly and wonderfully surprised at \"O' B...      1   \n",
       "4  This movie spends most of its time preaching t...      0   \n",
       "\n",
       "                                        vader_scores  \n",
       "0  {'neg': 0.069, 'neu': 0.788, 'pos': 0.143, 'co...  \n",
       "1  {'neg': 0.066, 'neu': 0.862, 'pos': 0.073, 'co...  \n",
       "2  {'neg': 0.24, 'neu': 0.583, 'pos': 0.177, 'com...  \n",
       "3  {'neg': 0.075, 'neu': 0.752, 'pos': 0.173, 'co...  \n",
       "4  {'neg': 0.066, 'neu': 0.707, 'pos': 0.227, 'co...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see\n",
    "pd_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3e7c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab final sentiment\n",
    "pd_test[\"sentiment_vader\"]=pd_test[\"vader_scores\"].apply(lambda x: np.where(x[\"compound\"] > 0, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a79ee1",
   "metadata": {},
   "source": [
    "Now that we have performed the classification task, we can see compare the labels and our predictions. We will be using a simple accuracy measure of how many labels were correctly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "904ab613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      {'neg': 0.069, 'neu': 0.788, 'pos': 0.143, 'co...\n",
       "1      {'neg': 0.066, 'neu': 0.862, 'pos': 0.073, 'co...\n",
       "2      {'neg': 0.24, 'neu': 0.583, 'pos': 0.177, 'com...\n",
       "3      {'neg': 0.075, 'neu': 0.752, 'pos': 0.173, 'co...\n",
       "4      {'neg': 0.066, 'neu': 0.707, 'pos': 0.227, 'co...\n",
       "                             ...                        \n",
       "295    {'neg': 0.059, 'neu': 0.812, 'pos': 0.129, 'co...\n",
       "296    {'neg': 0.056, 'neu': 0.87, 'pos': 0.074, 'com...\n",
       "297    {'neg': 0.056, 'neu': 0.825, 'pos': 0.119, 'co...\n",
       "298    {'neg': 0.092, 'neu': 0.768, 'pos': 0.14, 'com...\n",
       "299    {'neg': 0.083, 'neu': 0.785, 'pos': 0.132, 'co...\n",
       "Name: vader_scores, Length: 300, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_test['vader_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7078cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6966666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(pd_test['label'], pd_test['sentiment_vader'])\n",
    "\n",
    "# see\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02936f17",
   "metadata": {},
   "source": [
    "### Training a Machine Learning Classifier\n",
    "\n",
    "The next step to try if your dictionary is not working well is to train your own machine learning classifier. To build a simple Machine Learning classifier you will need to combine two different processes: \n",
    "\n",
    "- Build your input: here you will do all the steps we saw before  to convert text to numbers. The goal here is to use a document feature matrix as the input for the ML model. \n",
    "\n",
    "- Use `sklearn` to train your model and assess its accuracy. \n",
    "\n",
    "As before, let's not dig deep on the differences between each model.  We will learn how to train a model using a logistic regression with penalized terms. You can use the same code with `sklearn` to try different models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34229f70",
   "metadata": {},
   "source": [
    "### Building you input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff98bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre processing steps\n",
    "# stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# stemming\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b01298df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    # increase stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words = stop_words + [\"https\", \"rt\", \"amp\"]\n",
    "    \n",
    "    # tokenization \n",
    "    tokens_ = word_tokenize(text)\n",
    "    \n",
    "    # Generate a list of tokens after preprocessing\n",
    " \n",
    "    # normalize\n",
    "    tokens_ = [word.lower() for word in tokens_ if word.isalpha()]\n",
    "\n",
    "    # stem and stopwords\n",
    "    \n",
    "    # instatiate the stemmer\n",
    "    porter = PorterStemmer()\n",
    "\n",
    "    tokens_ =  [porter.stem(word) for word in tokens_ if word not in stop_words]\n",
    "    # Return the preprocessed tokens as a string\n",
    "    return tokens_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "348b60e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is no relation at all between Fortier an...</td>\n",
       "      <td>1</td>\n",
       "      <td>[relat, fortier, profil, fact, polic, seri, vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie is a great. The plot is very true t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[movi, great, plot, true, book, classic, writt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>George P. Cosmatos' \"Rambo: First Blood Part I...</td>\n",
       "      <td>0</td>\n",
       "      <td>[georg, cosmato, rambo, first, blood, part, ii...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the process of trying to establish the audi...</td>\n",
       "      <td>1</td>\n",
       "      <td>[process, tri, establish, audienc, empathi, ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yeh, I know -- you're quivering with excitemen...</td>\n",
       "      <td>0</td>\n",
       "      <td>[yeh, know, quiver, excit, well, secret, live,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  There is no relation at all between Fortier an...      1   \n",
       "1  This movie is a great. The plot is very true t...      1   \n",
       "2  George P. Cosmatos' \"Rambo: First Blood Part I...      0   \n",
       "3  In the process of trying to establish the audi...      1   \n",
       "4  Yeh, I know -- you're quivering with excitemen...      0   \n",
       "\n",
       "                                              tokens  \n",
       "0  [relat, fortier, profil, fact, polic, seri, vi...  \n",
       "1  [movi, great, plot, true, book, classic, writt...  \n",
       "2  [georg, cosmato, rambo, first, blood, part, ii...  \n",
       "3  [process, tri, establish, audienc, empathi, ja...  \n",
       "4  [yeh, know, quiver, excit, well, secret, live,...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the same pre-process function we used for topic models\n",
    "# notice we are working with the training data\n",
    "pd_train[\"tokens\"] = pd_train[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# let's see\n",
    "pd_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "751123b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeate for test\n",
    "pd_test[\"tokens\"] = pd_test[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02f7bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all\n",
    "pd_train[\"tokens\"] = pd_train[\"tokens\"].apply(' '.join)\n",
    "pd_test[\"tokens\"] = pd_test[\"tokens\"].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5350133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 5608)\n",
      "(300, 5608)\n"
     ]
    }
   ],
   "source": [
    "# lets build our document feature matrix using Tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# instantiate a vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# get tfidf\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    min_df=5, max_df=.90\n",
    ")\n",
    "\n",
    "# transform\n",
    "train_tfidf = vectorizer.fit_transform(pd_train[\"tokens\"]) # transform train\n",
    "test_tfidf = vectorizer.transform(pd_test[\"tokens\"]) # transform test\n",
    "\n",
    "# check\n",
    "print(train_tfidf.shape)\n",
    "print(test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99e0274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the targer\n",
    "y_train = pd_train[\"label\"]\n",
    "y_test = pd_test[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f5a349",
   "metadata": {},
   "source": [
    "### Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fe92727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(penalty=&#x27;l1&#x27;, random_state=42, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(penalty=&#x27;l1&#x27;, random_state=42, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(penalty='l1', random_state=42, solver='liblinear')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# train the model\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
    "model.fit(train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a7eea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8366666666666667\n",
      "Confusion Matrix:\n",
      " [[121  29]\n",
      " [ 20 130]]\n"
     ]
    }
   ],
   "source": [
    "# assess the model\n",
    "y_pred = model.predict(test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d0357",
   "metadata": {},
   "source": [
    "Pretty cool accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70c3f0a",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "Modify the code above to try a different model. You can either use different parameters for the logistic regression or just try a different model. Did you do better than my baseline model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c993455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffa77a7",
   "metadata": {},
   "source": [
    "## Pre-Trained Large Language Models: Hugging Face\n",
    "\n",
    "In the past few years, the field of natural language processing  has undergone through a major revolution. As we first saw, the early generation of NLP models was based on the idea of converting text to numbers through the use of document-feature matrix relying on the bag-of-words assumptions. \n",
    "\n",
    "In the past ten-years, we have seen the emergence of a new paradigm using deep-learning and neural networks models to improve on the representation of text as numbers. These new models move away from the idea of a bag-of-words towards a more refined representation of text capturing the contextual meaning of words and sentences. This is achieved by training models with billions of parameters on text-sequencing tasks, using as inputs a dense representation of words. These are the famous word embeddings. \n",
    "\n",
    "The most recent innovation on this revolution has been the Transformers Models. These models use multiple embeddings (matrices) to represent word, in which each matrix can capture different contextual representations of words. This dynamic representation allow for higher predictive power on downstream tasks in which these matrices form the foundation of the entire machine learning architecture. For example, Transformers are the the core of the language models like Open AI's GPTs and Meta's LLaMa.\n",
    "\n",
    "The Transformers use a sophisticated architecture that requires a huge amount of data and computational power to be trained. However, several of these models are open-sourced and are made available for us on the web through a platform called [Hugging Face](https://huggingface.co/). Those are what we call **pre-trained large language models**. At this point, there are thousands of pre-trained models based on the transformers framework available at hugging face. \n",
    "\n",
    "Once you find a model that fits your task, you have two options: \n",
    "\n",
    "- **Use the model architecture: access the model through the transformers library, and use it in you predictive tasks.** \n",
    "\n",
    "- **Fine-Tunning:** this is the most traditional way. You will get the model, give some data, re-train the model slightly so that the model will learn patterns from your data, and use on your predictive task. By fine-tuning a Transformers-based model for our own application, we can improve contextual understanding and therefore task-specific performance\n",
    "\n",
    "We will see example of the first for sentiment analysis. If you were to do build a full pipeline for classification, you would probably need to fine-tune the model. To learn more about fine-tunning, I suggest you to read: \n",
    "\n",
    "- here on hugging face: https://huggingface.co/blog/sentiment-analysis-python\n",
    "\n",
    "- and this forthcoming paper for political science applications:https://joantimoneda.netlify.app/files/Timoneda%20Vallejo%20V%20JOP.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbba8e6",
   "metadata": {},
   "source": [
    "### Transformers Library\n",
    "\n",
    "To use a model available on hugging face, you only need a few lines of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a34c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pipeline function\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25f8f8",
   "metadata": {},
   "source": [
    "Use the pipeline class to access the model. The pipeline function will give you the default model for this task, that in this case is a Bert-Based Model, see here: https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english?text=I+like+you.+I+love+you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "884670d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# instantiate your model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1e1d58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_classification.TextClassificationPipeline at 0x2bf4916d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the model\n",
    "sentiment_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1cec2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, I loved the Data Science I course. The best course I have ever done! DS I was ok. Professor Ventura jokes were not great\n"
     ]
    }
   ],
   "source": [
    "# see simple cases\n",
    "print(review1, review2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bc18f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998651742935181},\n",
       " {'label': 'NEGATIVE', 'score': 0.6337788105010986}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction\n",
    "sentiment_pipeline([review1, review2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a129d",
   "metadata": {},
   "source": [
    "We can easily use this model to make predictions on our entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3512b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict in the entire model. \n",
    "# notice here I am truncating the model. Transformers can only deal with 512 tokens max\n",
    "pd_test[\"bert_scores\"]=pd_test[\"text\"].apply(sentiment_pipeline, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca2ea9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's clean it up\n",
    "pd_test[\"bert_class\"]=pd_test[\"bert_scores\"].apply(lambda x: np.where(x[0][\"label\"]==\"POSITIVE\", 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78a54178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>vader_scores</th>\n",
       "      <th>sentiment_vader</th>\n",
       "      <th>tokens</th>\n",
       "      <th>bert_scores</th>\n",
       "      <th>bert_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;When I unsuspectedly rented A Thou...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.069, 'neu': 0.788, 'pos': 0.143, 'co...</td>\n",
       "      <td>1</td>\n",
       "      <td>br br unsuspectedli rent thousand acr thought ...</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.998875796794...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is the latest entry in the long series of...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.066, 'neu': 0.862, 'pos': 0.073, 'co...</td>\n",
       "      <td>1</td>\n",
       "      <td>latest entri long seri film french agent frenc...</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.996983110904...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie was so frustrating. Everything seem...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'neg': 0.24, 'neu': 0.583, 'pos': 0.177, 'com...</td>\n",
       "      <td>0</td>\n",
       "      <td>movi frustrat everyth seem energet total prepa...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.997244238853...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was truly and wonderfully surprised at \"O' B...</td>\n",
       "      <td>1</td>\n",
       "      <td>{'neg': 0.075, 'neu': 0.752, 'pos': 0.173, 'co...</td>\n",
       "      <td>1</td>\n",
       "      <td>truli wonder surpris brother art thou video st...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.649209976196...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This movie spends most of its time preaching t...</td>\n",
       "      <td>0</td>\n",
       "      <td>{'neg': 0.066, 'neu': 0.707, 'pos': 0.227, 'co...</td>\n",
       "      <td>1</td>\n",
       "      <td>movi spend time preach script make movi appar ...</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.998503446578...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  <br /><br />When I unsuspectedly rented A Thou...      1   \n",
       "1  This is the latest entry in the long series of...      1   \n",
       "2  This movie was so frustrating. Everything seem...      0   \n",
       "3  I was truly and wonderfully surprised at \"O' B...      1   \n",
       "4  This movie spends most of its time preaching t...      0   \n",
       "\n",
       "                                        vader_scores  sentiment_vader  \\\n",
       "0  {'neg': 0.069, 'neu': 0.788, 'pos': 0.143, 'co...                1   \n",
       "1  {'neg': 0.066, 'neu': 0.862, 'pos': 0.073, 'co...                1   \n",
       "2  {'neg': 0.24, 'neu': 0.583, 'pos': 0.177, 'com...                0   \n",
       "3  {'neg': 0.075, 'neu': 0.752, 'pos': 0.173, 'co...                1   \n",
       "4  {'neg': 0.066, 'neu': 0.707, 'pos': 0.227, 'co...                1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  br br unsuspectedli rent thousand acr thought ...   \n",
       "1  latest entri long seri film french agent frenc...   \n",
       "2  movi frustrat everyth seem energet total prepa...   \n",
       "3  truli wonder surpris brother art thou video st...   \n",
       "4  movi spend time preach script make movi appar ...   \n",
       "\n",
       "                                         bert_scores  bert_class  \n",
       "0  [{'label': 'POSITIVE', 'score': 0.998875796794...           1  \n",
       "1  [{'label': 'POSITIVE', 'score': 0.996983110904...           1  \n",
       "2  [{'label': 'NEGATIVE', 'score': 0.997244238853...           0  \n",
       "3  [{'label': 'NEGATIVE', 'score': 0.649209976196...           0  \n",
       "4  [{'label': 'NEGATIVE', 'score': 0.998503446578...           0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93056924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87\n"
     ]
    }
   ],
   "source": [
    "## accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(pd_test['label'], pd_test['bert_class'])\n",
    "# see\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba586ce",
   "metadata": {},
   "source": [
    "Without any fine-tunning, we are already doing much, much better than dictionaries!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7a3e5",
   "metadata": {},
   "source": [
    "### Use contextual knowledge: Model Trained on Amazon Reviews\n",
    "\n",
    "Since I do not want go to the in-depth process of fine-tunning your model, let's see if there are models on Hugging Face that were actually trained on a similar task: predicting reviews. \n",
    "\n",
    "Actually, there are many. See here: https://huggingface.co/models?sort=trending&search=sentiment+reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d13f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# acessing the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"MICADEE/autonlp-imdb-sentiment-analysis2-7121569\")\n",
    "\n",
    "# Acessing the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MICADEE/autonlp-imdb-sentiment-analysis2-7121569\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f68d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use in my model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Run in the dataframe\n",
    "pd_test[\"imdb_scores\"]=pd_test[\"text\"].apply(sentiment_pipeline, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59a6ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean\n",
    "pd_test[\"imdb_class\"]=pd_test[\"imdb_scores\"].apply(lambda x: np.where(x[0][\"label\"]==\"positive\", 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3aa711ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n"
     ]
    }
   ],
   "source": [
    "## accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(pd_test['label'], pd_test['imdb_class'])\n",
    "# see\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df165749",
   "metadata": {},
   "source": [
    "### Outsourcing to Generative Text-Based Models\n",
    "\n",
    "The last thing I will show you in class is the possibility of using ChatGPT as a classification tool. As you know, ChatGPT is an large language model (as we just saw) developed by OpenAI, based on the GPT architecture. The model was trained on a word-prediction task and it has blown the world by its capacity to engage in conversational interactions.\n",
    "\n",
    "Some recent papers have shown ChatGPT exhbits a strong performance on downstream classification tasks, like sentiment analysis, even though the model has not been trained or even fine-tuned for this task. Read here:https://osf.io/preprints/psyarxiv/sekf5/\n",
    "\n",
    "In this paper, there is availabe code in R on how to interact with ChatGPT API. The example I show you below pretty much converts their code to Python. You can see a nice video showing their R code here: https://www.youtube.com/watch?v=Mm3uoK4Fogc\n",
    "\n",
    "The whole process requires us to have access to the Open AI API which allow us to query continously the GPT models. Notice, this is not free. You pay for every query. That being said, it is quite cheap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12b2bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load api key\n",
    "# load library to get environmental files\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# load keys from  environmental var\n",
    "load_dotenv() # .env file in cwd\n",
    "gpt_key = os.environ.get(\"gpt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e1b3d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "# define headers\n",
    "headers = {\n",
    "        \"Authorization\": f\"Bearer {gpt_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "# define gpt model\n",
    "question = \"Please, tell me more about the Data Science and Public Policy Program at Georgetown's McCourt School\"\n",
    "\n",
    "data = {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"temperature\": 0,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": question}]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# send a post request\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", \n",
    "                             json=data, \n",
    "                             headers=headers)\n",
    "# convert to json\n",
    "response_json = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2586732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-AXpZexgEXoUCbLi0PTVcF8odqvjtO',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1732626438,\n",
       " 'model': 'gpt-4-0613',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \"The Data Science and Public Policy (DSPP) program at Georgetown University's McCourt School of Public Policy is a unique program that combines traditional public policy studies with cutting-edge technical skills in data science. The program is designed to equip students with the necessary skills to develop, assess, and execute complex public policies using data-driven decision making.\\n\\nThe curriculum of the DSPP program includes courses in statistics, economics, computer science, and public policy. Students learn how to use data to analyze policy issues, make policy recommendations, and evaluate policy outcomes. They also learn how to communicate their findings effectively to policymakers and the public.\\n\\nThe program is designed for students who have a strong interest in public policy and a desire to use data science to make a positive impact on society. Graduates of the program are prepared for careers in government, non-profit organizations, and the private sector where they can use their skills to inform policy decisions and improve public services.\\n\\nThe DSPP program is a two-year, full-time program. Students are required to complete a capstone project in their second year, where they work with a real-world client to solve a policy problem using data science.\\n\\nThe program also offers opportunities for students to gain practical experience through internships and research projects. Students have the opportunity to work with faculty members on research projects, and the program has partnerships with various organizations that offer internships to students.\\n\\nIn addition to the technical skills, the program also emphasizes the ethical considerations of using data in policy making, ensuring that students are prepared to use data responsibly and effectively in their careers.\",\n",
       "    'refusal': None},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 26,\n",
       "  'completion_tokens': 313,\n",
       "  'total_tokens': 339,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0}},\n",
       " 'system_fingerprint': None}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see the output\n",
    "response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6becdd09",
   "metadata": {},
   "source": [
    "Let's now write a function to query the api at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca381f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interact with the ChatGPT API\n",
    "def hey_chatGPT(question_text, api_key):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"temperature\": 0,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": question_text}]\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", \n",
    "                             json=data, \n",
    "                             headers=headers, timeout=5)\n",
    "    \n",
    "    response_json = response.json()\n",
    "    return response_json['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66bbf297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "output = []\n",
    "# Run a loop over your dataset of reviews and prompt ChatGPT\n",
    "for i in range(len(pd_test)):\n",
    "    try: \n",
    "        print(i)\n",
    "        question = \"Is the sentiment of this text positive, neutral, or negative? \\\n",
    "        Answer only with a number: 1 if positive, 0 if neutral or negative. \\\n",
    "        Here is the text: \"\n",
    "        text = pd_test.loc[i, \"text\"]\n",
    "        full_question = question + str(text)\n",
    "        output.append(hey_chatGPT(full_question, gpt_key))\n",
    "    except:\n",
    "        output.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "374e6e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fdd386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add as a column\n",
    "pd_test[\"gpt_scores\"] = pd.to_numeric(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4e6da91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test2 = pd_test.dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4bb7f04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967479674796748\n"
     ]
    }
   ],
   "source": [
    "# check accuracy\n",
    "accuracy = accuracy_score(pd_test2['label'], pd_test2['gpt_scores'])\n",
    "# see\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552dcf76",
   "metadata": {},
   "source": [
    "Pretty good results! Notice, we have no fine-tunning here. Just grabing results from the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f00afce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook _week_12_nlp_II_onlysp.ipynb to html\n",
      "[NbConvertApp] Writing 355837 bytes to _week_12_nlp_II_onlysp.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert _week_12_nlp_II_onlysp.ipynb --to html --template classic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
